{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from keras.activations import softmax\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m is no. of training examples, each example has a sequence of Tx inputs\n",
    "Tx = 10\n",
    "Ty = 10\n",
    "frame_vals_len = 1805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax,name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
    "    e = densor(concat)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(e)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64\n",
    "n_s = 128\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(frame_vals_len, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, frame_vals_len):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, frame_vals_len))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_H = 19\n",
    "GRID_W = 19\n",
    "NO_OBJECT_SCALE  = 1.0\n",
    "OBJECT_SCALE     = 5.0\n",
    "COORD_SCALE      = 1.0\n",
    "CLASS_SCALE      = 1.0\n",
    "\n",
    "BATCH_SIZE       = 16\n",
    "WARM_UP_BATCHES  = 0\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    \n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 1, 1])\n",
    "    \n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    conf_mask  = tf.zeros(mask_shape)\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "    \n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., 1:3]) + cell_grid\n",
    "    \n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 3:5])\n",
    "    \n",
    "    ### adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 0])\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 1:3] # relative position to the containing cell\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 3:5] # number of cells accross, horizontally and vertically\n",
    "    \n",
    "    ### adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "    \n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "    \n",
    "    true_box_conf = iou_scores * y_true[..., 0]\n",
    "    \n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 0], axis=-1) * COORD_SCALE\n",
    "    \n",
    "    ### confidence mask: penelize predictors + penalnp_arr_gt.shapeize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = true_boxes[..., 0:2]\n",
    "    true_wh = true_boxes[..., 2:4]\n",
    "    \n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    np_arr_gt.shape\n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
    "    \n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "          \n",
    "    \n",
    "    \"\"\"\n",
    "    Warm-up training\n",
    "    \"\"\"\n",
    "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "    \n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
    "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
    "                                   tf.ones_like(coord_mask)],\n",
    "                          lambda: [true_box_xy, \n",
    "                                   true_box_wh,\n",
    "                                   coord_mask])\n",
    "    \n",
    "    \"\"\"\n",
    "    Finalize the loss\n",
    "    \"\"\"\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "    \n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
    "    \n",
    "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "\n",
    "    \"\"\"\n",
    "    Debugging code\n",
    "    \"\"\"    \n",
    "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "    total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    " \n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "        \n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    " \n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    " \n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    " \n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    " \n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "        \n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "        \n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "        \n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    " \n",
    "    # return only the bounding boxes that were picked using the\n",
    "    # integer data type\n",
    "    return pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_conf_layer(x):\n",
    "    return np.delete(x,0,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    lambda_coord = 5\n",
    "    lambda_noobj = 0.5\n",
    "    #non max suppression\n",
    "    threshold = 0.5\n",
    "    y_pred_conf = y_pred[...,0]\n",
    "    logical_mask = tf.greater_equal(y_pred_conf,threshold)\n",
    "    int_mask = tf.cast(logical_mask, tf.float32)\n",
    "    y_pred_conf = y_pred_conf*int_mask\n",
    "    y_pred = tf.cast(y_pred,tf.float32)\n",
    "    y_pred = tf.stack(y_pred_conf,y_pred[...,1:])\n",
    "    for i in range(10):\n",
    "        index_of_boxes = []\n",
    "        boxes_dims =[]\n",
    "        for j in range(19):\n",
    "            for k in range(19):\n",
    "                if y_pred[i,j,k,0]>0:\n",
    "                    index_of_boxes.append([i,j,k])\n",
    "                    xyhw = [(1/19)*(j+y_pred[i,j,k,1]), (1/19)*(k+y_pred[i,j,k,2]),(1/19)*(y_pred[i,j,k,3]),(1/19)*(y_pred[i,j,k,4])]\n",
    "                    xy2 = [xywh[0]-(xywh[3]/2),xywh[1]-(xywh[2]/2),xywh[0]+(xywh[3]/2),xywh[1]+(xywh[3]/2)]\n",
    "                    boxes_dims.append(xy2)\n",
    "        picked_boxes_indices = non_max_suppression_fast(boxes_dims, threshold)\n",
    "        for index in picked_boxes_indices:\n",
    "            picked_box = boxes_dims[index]\n",
    "            new_xyhw = [(picked_box[2] - picked_box[0])/2,(picked_box[3] - picked_box[1])/2,picked_box[3] - picked_box[1], picked_box[2] - picked_box[0]]\n",
    "            y_pred[i,int(new_xywh[0]/(1/19)),int(new_xywh[1]/(1/19)),1] = new_xyhw[0]/(1/19) - int(new_xyhw[0]/(1/19))\n",
    "            y_pred[i,int(new_xywh[0]/(1/19)),int(new_xywh[1]/(1/19)),2] = new_xyhw[1]/(1/19) - int(new_xyhw[1]/(1/19))\n",
    "            y_pred[i,int(new_xywh[0]/(1/19)),int(new_xywh[1]/(1/19)),3] = new_xyhw[2]/(1/19)\n",
    "            y_pred[i,int(new_xywh[0]/(1/19)),int(new_xywh[1]/(1/19)),4] = new_xyhw[3]/(1/19)\n",
    "            \n",
    "    #calculating loss using yolo loss function\n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., 1:3])\n",
    "    pred_box_hw = tf.exp(y_pred[..., 3:5])\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 0])\n",
    "    \n",
    "    true_box_xy = y_true[...,1:3]\n",
    "    true_box_hw = y_pred[...,3:5]\n",
    "    true_box_conf = y_pred[...,0]\n",
    "    \n",
    "    loss_xy = lambda_coord*np.sum(true_box_conf*np.square(true_box_xy-pred_box_xy))\n",
    "    loss_wh = lambda_coord*np.sum(true_box_conf*np.square(np.sqrt(true_box_hw)-np.sqrt(pred_box_hw)))\n",
    "    loss_conf = np.sum(true_box_conf*np.square(true_box_conf - pred_box_conf)) + lambda_noobj*np.sum(np.abs(true_box_conf-1)*np.square(true_box_conf - pred_box_conf))\n",
    "    loss = loss_xy + loss_wh + loss_conf\n",
    "    #debugging\n",
    "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 20,  21,  22,  23],\n",
       "         [ 24,  25,  26,  27],\n",
       "         [ 28,  29,  30,  31],\n",
       "         [ 32,  33,  34,  35],\n",
       "         [ 36,  37,  38,  39]],\n",
       "\n",
       "        [[ 40,  41,  42,  43],\n",
       "         [ 44,  45,  46,  47],\n",
       "         [ 48,  49,  50,  51],\n",
       "         [ 52,  53,  54,  55],\n",
       "         [ 56,  57,  58,  59]],\n",
       "\n",
       "        [[ 60,  61,  62,  63],\n",
       "         [ 64,  65,  66,  67],\n",
       "         [ 68,  69,  70,  71],\n",
       "         [ 72,  73,  74,  75],\n",
       "         [ 76,  77,  78,  79]],\n",
       "\n",
       "        [[ 80,  81,  82,  83],\n",
       "         [ 84,  85,  86,  87],\n",
       "         [ 88,  89,  90,  91],\n",
       "         [ 92,  93,  94,  95],\n",
       "         [ 96,  97,  98,  99]]],\n",
       "\n",
       "\n",
       "       [[[120, 121, 122, 123],\n",
       "         [124, 125, 126, 127],\n",
       "         [128, 129, 130, 131],\n",
       "         [132, 133, 134, 135],\n",
       "         [136, 137, 138, 139]],\n",
       "\n",
       "        [[140, 141, 142, 143],\n",
       "         [144, 145, 146, 147],\n",
       "         [148, 149, 150, 151],\n",
       "         [152, 153, 154, 155],\n",
       "         [156, 157, 158, 159]],\n",
       "\n",
       "        [[160, 161, 162, 163],\n",
       "         [164, 165, 166, 167],\n",
       "         [168, 169, 170, 171],\n",
       "         [172, 173, 174, 175],\n",
       "         [176, 177, 178, 179]],\n",
       "\n",
       "        [[180, 181, 182, 183],\n",
       "         [184, 185, 186, 187],\n",
       "         [188, 189, 190, 191],\n",
       "         [192, 193, 194, 195],\n",
       "         [196, 197, 198, 199]]],\n",
       "\n",
       "\n",
       "       [[[220, 221, 222, 223],\n",
       "         [224, 225, 226, 227],\n",
       "         [228, 229, 230, 231],\n",
       "         [232, 233, 234, 235],\n",
       "         [236, 237, 238, 239]],\n",
       "\n",
       "        [[240, 241, 242, 243],\n",
       "         [244, 245, 246, 247],\n",
       "         [248, 249, 250, 251],\n",
       "         [252, 253, 254, 255],\n",
       "         [256, 257, 258, 259]],\n",
       "\n",
       "        [[260, 261, 262, 263],\n",
       "         [264, 265, 266, 267],\n",
       "         [268, 269, 270, 271],\n",
       "         [272, 273, 274, 275],\n",
       "         [276, 277, 278, 279]],\n",
       "\n",
       "        [[280, 281, 282, 283],\n",
       "         [284, 285, 286, 287],\n",
       "         [288, 289, 290, 291],\n",
       "         [292, 293, 294, 295],\n",
       "         [296, 297, 298, 299]]],\n",
       "\n",
       "\n",
       "       [[[320, 321, 322, 323],\n",
       "         [324, 325, 326, 327],\n",
       "         [328, 329, 330, 331],\n",
       "         [332, 333, 334, 335],\n",
       "         [336, 337, 338, 339]],\n",
       "\n",
       "        [[340, 341, 342, 343],\n",
       "         [344, 345, 346, 347],\n",
       "         [348, 349, 350, 351],\n",
       "         [352, 353, 354, 355],\n",
       "         [356, 357, 358, 359]],\n",
       "\n",
       "        [[360, 361, 362, 363],\n",
       "         [364, 365, 366, 367],\n",
       "         [368, 369, 370, 371],\n",
       "         [372, 373, 374, 375],\n",
       "         [376, 377, 378, 379]],\n",
       "\n",
       "        [[380, 381, 382, 383],\n",
       "         [384, 385, 386, 387],\n",
       "         [388, 389, 390, 391],\n",
       "         [392, 393, 394, 395],\n",
       "         [396, 397, 398, 399]]],\n",
       "\n",
       "\n",
       "       [[[420, 421, 422, 423],\n",
       "         [424, 425, 426, 427],\n",
       "         [428, 429, 430, 431],\n",
       "         [432, 433, 434, 435],\n",
       "         [436, 437, 438, 439]],\n",
       "\n",
       "        [[440, 441, 442, 443],\n",
       "         [444, 445, 446, 447],\n",
       "         [448, 449, 450, 451],\n",
       "         [452, 453, 454, 455],\n",
       "         [456, 457, 458, 459]],\n",
       "\n",
       "        [[460, 461, 462, 463],\n",
       "         [464, 465, 466, 467],\n",
       "         [468, 469, 470, 471],\n",
       "         [472, 473, 474, 475],\n",
       "         [476, 477, 478, 479]],\n",
       "\n",
       "        [[480, 481, 482, 483],\n",
       "         [484, 485, 486, 487],\n",
       "         [488, 489, 490, 491],\n",
       "         [492, 493, 494, 495],\n",
       "         [496, 497, 498, 499]]],\n",
       "\n",
       "\n",
       "       [[[520, 521, 522, 523],\n",
       "         [524, 525, 526, 527],\n",
       "         [528, 529, 530, 531],\n",
       "         [532, 533, 534, 535],\n",
       "         [536, 537, 538, 539]],\n",
       "\n",
       "        [[540, 541, 542, 543],\n",
       "         [544, 545, 546, 547],\n",
       "         [548, 549, 550, 551],\n",
       "         [552, 553, 554, 555],\n",
       "         [556, 557, 558, 559]],\n",
       "\n",
       "        [[560, 561, 562, 563],\n",
       "         [564, 565, 566, 567],\n",
       "         [568, 569, 570, 571],\n",
       "         [572, 573, 574, 575],\n",
       "         [576, 577, 578, 579]],\n",
       "\n",
       "        [[580, 581, 582, 583],\n",
       "         [584, 585, 586, 587],\n",
       "         [588, 589, 590, 591],\n",
       "         [592, 593, 594, 595],\n",
       "         [596, 597, 598, 599]]],\n",
       "\n",
       "\n",
       "       [[[620, 621, 622, 623],\n",
       "         [624, 625, 626, 627],\n",
       "         [628, 629, 630, 631],\n",
       "         [632, 633, 634, 635],\n",
       "         [636, 637, 638, 639]],\n",
       "\n",
       "        [[640, 641, 642, 643],\n",
       "         [644, 645, 646, 647],\n",
       "         [648, 649, 650, 651],\n",
       "         [652, 653, 654, 655],\n",
       "         [656, 657, 658, 659]],\n",
       "\n",
       "        [[660, 661, 662, 663],\n",
       "         [664, 665, 666, 667],\n",
       "         [668, 669, 670, 671],\n",
       "         [672, 673, 674, 675],\n",
       "         [676, 677, 678, 679]],\n",
       "\n",
       "        [[680, 681, 682, 683],\n",
       "         [684, 685, 686, 687],\n",
       "         [688, 689, 690, 691],\n",
       "         [692, 693, 694, 695],\n",
       "         [696, 697, 698, 699]]],\n",
       "\n",
       "\n",
       "       [[[720, 721, 722, 723],\n",
       "         [724, 725, 726, 727],\n",
       "         [728, 729, 730, 731],\n",
       "         [732, 733, 734, 735],\n",
       "         [736, 737, 738, 739]],\n",
       "\n",
       "        [[740, 741, 742, 743],\n",
       "         [744, 745, 746, 747],\n",
       "         [748, 749, 750, 751],\n",
       "         [752, 753, 754, 755],\n",
       "         [756, 757, 758, 759]],\n",
       "\n",
       "        [[760, 761, 762, 763],\n",
       "         [764, 765, 766, 767],\n",
       "         [768, 769, 770, 771],\n",
       "         [772, 773, 774, 775],\n",
       "         [776, 777, 778, 779]],\n",
       "\n",
       "        [[780, 781, 782, 783],\n",
       "         [784, 785, 786, 787],\n",
       "         [788, 789, 790, 791],\n",
       "         [792, 793, 794, 795],\n",
       "         [796, 797, 798, 799]]],\n",
       "\n",
       "\n",
       "       [[[820, 821, 822, 823],\n",
       "         [824, 825, 826, 827],\n",
       "         [828, 829, 830, 831],\n",
       "         [832, 833, 834, 835],\n",
       "         [836, 837, 838, 839]],\n",
       "\n",
       "        [[840, 841, 842, 843],\n",
       "         [844, 845, 846, 847],\n",
       "         [848, 849, 850, 851],\n",
       "         [852, 853, 854, 855],\n",
       "         [856, 857, 858, 859]],\n",
       "\n",
       "        [[860, 861, 862, 863],\n",
       "         [864, 865, 866, 867],\n",
       "         [868, 869, 870, 871],\n",
       "         [872, 873, 874, 875],\n",
       "         [876, 877, 878, 879]],\n",
       "\n",
       "        [[880, 881, 882, 883],\n",
       "         [884, 885, 886, 887],\n",
       "         [888, 889, 890, 891],\n",
       "         [892, 893, 894, 895],\n",
       "         [896, 897, 898, 899]]],\n",
       "\n",
       "\n",
       "       [[[920, 921, 922, 923],\n",
       "         [924, 925, 926, 927],\n",
       "         [928, 929, 930, 931],\n",
       "         [932, 933, 934, 935],\n",
       "         [936, 937, 938, 939]],\n",
       "\n",
       "        [[940, 941, 942, 943],\n",
       "         [944, 945, 946, 947],\n",
       "         [948, 949, 950, 951],\n",
       "         [952, 953, 954, 955],\n",
       "         [956, 957, 958, 959]],\n",
       "\n",
       "        [[960, 961, 962, 963],\n",
       "         [964, 965, 966, 967],\n",
       "         [968, 969, 970, 971],\n",
       "         [972, 973, 974, 975],\n",
       "         [976, 977, 978, 979]],\n",
       "\n",
       "        [[980, 981, 982, 983],\n",
       "         [984, 985, 986, 987],\n",
       "         [988, 989, 990, 991],\n",
       "         [992, 993, 994, 995],\n",
       "         [996, 997, 998, 999]]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.reshape(np.arange(1000),(10,5,5,4))\n",
    "mask = np.greater(a[...,0],100*np.ones((a[...,0].shape)).astype(int))\n",
    "a[...,0] = a[...,0]*mask\n",
    "a[0,0,0] = [7,8,9,10]\n",
    "b = np.reshape(np.arange(1000),(10,5,5,4))\n",
    "np.delete(b,0,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2.9\n",
    "int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, 1805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10, 1805)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 10, 128)      957440      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 10, 128)      0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10, 256)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_2[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10, 1)        257         concatenate_2[0][0]              \n",
      "                                                                 concatenate_2[1][0]              \n",
      "                                                                 concatenate_2[2][0]              \n",
      "                                                                 concatenate_2[3][0]              \n",
      "                                                                 concatenate_2[4][0]              \n",
      "                                                                 concatenate_2[5][0]              \n",
      "                                                                 concatenate_2[6][0]              \n",
      "                                                                 concatenate_2[7][0]              \n",
      "                                                                 concatenate_2[8][0]              \n",
      "                                                                 concatenate_2[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 10, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_2[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_2[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_2[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_2[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_2[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_2[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_2[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_2[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_2[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_2[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1805)         232845      lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,322,126\n",
      "Trainable params: 1,322,126\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-3c1777de9018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m out = model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n\u001b[1;32m      2\u001b[0m                     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     loss=my_loss)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 342\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-8702f574af3f>\u001b[0m in \u001b[0;36mmy_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_pred_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_conf\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mint_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mindex_of_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1000\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvalue_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0mexpanded_num_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mexpanded_num_dims\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mexpanded_num_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m       raise ValueError(\"axis = %d not in [%d, %d)\" % (axis, -expanded_num_dims,\n\u001b[1;32m   1004\u001b[0m                                                       expanded_num_dims))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \"\"\"\n\u001b[0;32m--> 653\u001b[0;31m     raise TypeError(\"Using a `tf.Tensor` as a Python `bool` is not allowed. \"\n\u001b[0m\u001b[1;32m    654\u001b[0m                     \u001b[0;34m\"Use `if t is not None:` instead of `if t:` to test if a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;34m\"tensor is defined, and use TensorFlow ops such as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor."
     ]
    }
   ],
   "source": [
    "out = model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
    "                    metrics=['accuracy'],\n",
    "                    loss=my_loss)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
